# -*- coding: utf-8 -*-
"""빅종설baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19T-qluHSIFuURwyGg89rHZPIrQuQ20I3
"""

# 구글 드라이브 마운트
from google.colab import drive
drive.mount('/content/drive')

import os
import sys
from glob import glob
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms

from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split

import matplotlib
import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow
from PIL import Image
from tqdm import tqdm

import albumentations as A
from albumentations.pytorch import ToTensorV2 #!pip install albumentations==0.4.6

# image directory
img_dir = './drive/Shareddrives/빅종설/image'

# class, hyper-parameter
class_dict = {'joy':0, 'love':1, 'stress':2, 'missing':3, 'sad':4, 'heal':5, 'omg':6}

num_classes = 7
in_channel = 3
batch_size = 50
learning_rate = 0.001
num_epochs = 20
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# dataset
class CustomDataset(Dataset):
    
    image_paths = []
    emotion_labels = []
    
    def __init__(self, img_dir, transform=None):
        self.img_dir = img_dir
        self.transform = transform
        self.setup()

    def setup(self):
        emotions = os.listdir(self.img_dir)
        for emotion in emotions:
            image_paths = os.listdir(os.path.join(self.img_dir, emotion))
            
            for image_path in image_paths:
                self.image_paths.append(os.path.join(emotion, image_path))
                self.emotion_labels.append(class_dict[emotion])
                
    def __getitem__(self, index):
        image_path = self.image_paths[index]
        img = Image.open(os.path.join(img_dir, image_path)).convert('RGB')
        
        emotion_label = self.emotion_labels[index]
        
        img = self.transform(img)
        
        return img, emotion_label
    
    def __len__(self):
        return len(self.image_paths)

# augmentation
basic_transform = transforms.Compose([
                                      transforms.ToTensor(),
                                      #transforms.Resize(224),
                                      #transforms.Normalize(mean=(0.5,), std=(0.5,))
                                 ])

# train, valid dataset
basic_dataset = CustomDataset(img_dir=img_dir, transform=basic_transform)

n_val = int(len(basic_dataset) * 0.2)
n_train = len(basic_dataset) - n_val
train_dataset, val_dataset = torch.utils.data.random_split(basic_dataset, [n_train, n_val])

# data loader
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=False
)

# model
from torchvision.models import resnet18
model = resnet18(pretrained=True)
model.fc = nn.Linear(in_features=512, out_features=num_classes)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# train
loss_list = []
model.to(device)
for epoch in tqdm(range(num_epochs)):
    for i, (images, labels) in enumerate(train_loader):
        images = images.to(device).float()
        labels = labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)

        loss = criterion(outputs, labels)
        
        loss.backward()
        optimizer.step()
    loss_list.append(loss)

plt.plot(loss_list)

# validation
## validation set에 대한 정확도 표시
acc = 0
for i, (images, labels) in enumerate(val_loader):
    images = images.to(device).float()
    labels = labels.to(device)

    outputs = model(images)

    _, pred = torch.max(outputs.data, 1)
    acc += (pred == labels).sum().item()
print(acc/len(val_dataset)

# inference
## 원하는 이미지를 아래 이미지 경로에 넣으면 0~6 추론
### my_img = Image.open('이미지 경로').convert('RGB')
my_img = Image.open(os.path.join('./drive/Shareddrives/빅종설/flicker크롤링', 'missing/missing_00001.jpg')).convert('RGB')
tf = transforms.ToTensor()
my_img = tf(my_img)
my_img = my_img.unsqueeze(0)
my_img = my_img.to(device)
output = model(my_img)
_, pred = torch.max(output.data, 1)
print(pred)
print(output.data)